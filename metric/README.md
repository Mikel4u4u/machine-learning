# Метрические алгоритмы классификации

## Алгоритм k ближайших соседей – **kNN**

### Выбор данных

Выборка состоит из 150 экземпляров ирисов трех видов, имеет четыре характеристики: длина и ширина чашелистика (`Sepal.Length` и `Sepal.Width`), длина и ширина лепестка (`Petal.Length` и `Petal.Width`)

![Iris](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)

### Евклидово расстояние

$r(u, v) = (\sum_{i=1}^{n} {|u_i - v_i|}^{2})^{1/2}$

```R
euclideanDistance <- function(u, v) {
 sqrt(sum((u - v)^2))
}
```
### Функция kNN

**Пример:**
```
kNN(train = trainIris, test = testIris, cl = trainIris$Species, k = 3)
```

**Аргументы:**

`train` - обучающая выборка

`test` - контрольная выборка

`cl` - ответы (идентификаторы классов) обучающей выборки

`k` - колличество ближайших соседей, которое используется

Возвращает предсказания на тестовой выборке

### Критерий скользящего контроля LOO для kNN

Оптимальное в смысле точности предсказаний значение  `k`  может быть найдено с использованием перекрестной проверки. Для этого по фиксированному значению  `k`  строится модель  `k` -ближайших соседей и оценивается CV-ошибка классификации. Эти действия повторяются для различных  `k`  и значение, соответствующее наименьшей ошибке распознавания, принимается как оптимальное.

![LOO kNN](graphics/LOOknn.png)

### Карта	классификации kNN
---

## Алгоритм	k взвешенных	ближайших	соседей	– **kwNN**

### Критерий	скользящего	контроля	LOO для kwNN

### Карта	классификации	kwNN
---

## Сравнение	качества	алгоритмов	kNN и	kwNN.

## Пример,	показывающий	преимущество	метода kwNN над kNN.
	


