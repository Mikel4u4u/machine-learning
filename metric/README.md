# Метрические алгоритмы классификации

Метрические классификаторы опираются на гипотезу компактности, которая предполагает, что схожие объекты чаще лежат в одном классе, чем в разных. Для формализации понятия сходства вводится функция расстояния между объектами.

## Алгоритм k ближайших соседей – **kNN**

Пусть задана обучающая выборка пар «объект-ответ» ![](https://latex.codecogs.com/svg.latex?%5Clarge%20X%5El%20%3D%20%28x_i%2C%20y_i%29_%7Bi%3D1%7D%5El)

Пусть на множестве объектов задана функция расстояния ![](https://latex.codecogs.com/svg.latex?%5Clarge%20%5Crho%28x%2Cx%27%29) . Эта функция должна быть достаточно адекватной моделью сходства объектов. Чем больше значение этой функции, тем менее схожими являются два объекта (x,x'). 

Для произвольного объекта u расположим объекты обучающей выборки ![](https://latex.codecogs.com/svg.latex?%5Clarge%20x_i) в порядке возрастания расстояний до u:

![](https://latex.codecogs.com/svg.latex?%5Clarge%20%5Crho%28u%2C%20x_%7B1%3Bu%7D%29%20%5Cleqslant%20%5Crho%28u%2C%20x_%7B2%3Bu%7D%29%5Cleqslant%20...%5Cleqslant%20%5Crho%28u%2C%20x_%7Bl%3Bu%7D%29),

где через ![](https://latex.codecogs.com/svg.latex?%5Clarge%20x_%7Bi%3B%20u%7D) обозначается тот объект обучающей выборки, который является i-м соседом объекта u. 

Алгоритм ближайших соседей в общем виде:

![](https://latex.codecogs.com/svg.latex?%5Clarge%20a%28u%29%20%3D%20%5Cmathrm%7Barg%7D%5Cmax_%7By%5Cin%20Y%7D%20%5Csum_%7Bi%3D1%7D%5Em%20%5Cbigl%5B%20x_%7Bi%3B%20u%7D%3Dy%20%5Cbigr%5D%20w%28i%2Cu%29%2C)

![](https://latex.codecogs.com/svg.latex?%5Clarge%20w%28i%2Cu%29) - весовая функция.

Для kNN    ![](https://latex.codecogs.com/svg.latex?%5Clarge%20w%28i%2Cu%29%20%3D%20%5Bi%5Cleq%20k%5D)



### Выбор данных

Выборка состоит из 150 экземпляров ирисов трех видов, имеет четыре характеристики: длина и ширина чашелистика (`Sepal.Length` и `Sepal.Width`), длина и ширина лепестка (`Petal.Length` и `Petal.Width`)

![Iris](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)

### Евклидово расстояние

![](https://latex.codecogs.com/svg.latex?%5Cfn_phv%20%5Crho%28u%2C%20v%29%20%3D%20%28%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%7B%7Cu_i%20-%20v_i%7C%7D%5E%7B2%7D%29%5E%7B1/2%7D)

```R
euclideanDistance <- function(u, v) {
 sqrt(sum((u - v)^2))
}
```
### Функция kNN

Для классификации каждого из объектов тестовой выборки необходимо последовательно выполнить следующие операции:
* Вычислить расстояние до каждого из объектов обучающей выборки
* Отобрать k объектов обучающей выборки, расстояние до которых минимально
* Класс классифицируемого объекта — это класс, наиболее часто встречающийся среди k ближайших соседей

**Пример:**
```
kNN(train = trainIris, test = testIris, cl = trainIris$Species, k = 3)
```

**Аргументы:**

`train` - обучающая выборка

`test` - контрольная выборка

`cl` - ответы (идентификаторы классов) обучающей выборки

`k` - колличество ближайших соседей, которое используется

Возвращает предсказания на тестовой выборке

### Критерий скользящего контроля LOO для kNN

Оптимальное в смысле точности предсказаний значение  `k`  может быть найдено с использованием перекрестной проверки. Для этого по фиксированному значению  `k`  строится модель  `k` -ближайших соседей и оценивается ошибка классификации. Эти действия повторяются для различных  `k`  и значение, соответствующее наименьшей ошибке распознавания, принимается как оптимальное.

![LOO kNN](graphics/LOOknn.png)

### Карта	классификации kNN

![kNN](graphics/kNN.png)
---

## Алгоритм	k взвешенных	ближайших	соседей	– **kwNN**

i-му соседу объекта u приписывается вес ![](https://latex.codecogs.com/svg.latex?%5Clarge%20w%28i%2Cu%29), как правило, убывающий с ростом ранга соседа i. Объект относится к тому классу, который набирает больший суммарный вес среди k ближайших соседей.

Выберем следующую функцию веса ![](https://latex.codecogs.com/svg.latex?%5Clarge%20w_i%20%3D%5Cfrac%7B%28k%20&plus;%201-i%29%7D%7Bk%7D)
 
### Критерий	скользящего	контроля	LOO для kwNN

![LOO kNN](graphics/LOOkwnn.png)

### Карта	классификации	kwNN

![kNN](graphics/kwNN.png)
---

## Сравнение	качества	алгоритмов	kNN и	kwNN.

kNN — один из простейших алгоритмов классификации, поэтому на реальных задачах он зачастую оказывается неэффективным. Помимо точности классификации, проблемой этого классификатора является скорость классификации: если в обучающей выборке N объектов, в тестовой выборе M объектов, а размерность пространства — K, то количество операций для классификации тестовой выборки может быть оценено как O(K*M*N). 

kwNN отличается от kNN, тем что учитывает порядок соседей классифицируемого объекта, улчшая качество классификации.

## Пример,	показывающий	преимущество	метода kwNN над kNN.
	
Недостаток kNN в том, что максимальная сумма голосов может достигаться на нескольких классах одновременно.
В задачах с двумя классами этого можно избежать, если брать только нечётные значения k. Более общая тактика, которая годится и для случая многих классов — ввести строго убывающую последовательность вещественных весов, задающих вклад i-го соседа в классификацию.

## Метод парзеновского окна

Для оценки близости объекта u к классу y алгоритм использует следующую функцию:

 ![](https://latex.codecogs.com/svg.latex?%5Clarge%20W%28u%2Ci%29%3DK%28%5Cfrac%7B%5Crho%28u%2Cx_u%5E%7B%28i%29%7D%7D%7Bh%7D%29), где ![](https://latex.codecogs.com/svg.latex?%5Clarge%20W%28u%2Ci%29) — функция ядра.

Чаще всего применяются 5 типов ядер:

![](https://latex.codecogs.com/svg.latex?%5Clarge%20R%28z%29%3D%5Cfrac%7B1%7D%7B2%7D%5B%7Cz%5Cleq%20-1%5D) Прямоугольное 
![](https://latex.codecogs.com/svg.latex?%5Clarge%20T%28z%29%3D%281-%7Cz%7C%29%20%5Ccdot%20%5B%7Cz%5Cleq%201%5D) Треугольное 
![](https://camo.githubusercontent.com/1e5b0ae73cde8fe5b7a3c248bdbddc1639f96404/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c61726765253230512532387a2532392532302533442532302535436672616325374231352537442537423136253744253230253238312532302d2532307a253545322532392535453225323025354363646f742532302535422537437a2537432532302535436c657125323031253544)Квартическое 
![](https://camo.githubusercontent.com/0cad7a4e41913e389111d61935a76fba1cbff264/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c61726765253230452532387a253239253230253344253230253543667261632537423325374425374234253744253230253238312532302d2532307a2535453225323925323025354363646f742532302535422537437a2537432532302535436c657125323031253544)Епанечниково 
![]()Гауссовское (нормальное распределение)

### Карта	классификации	PW 
Гауссовское ядро:

![PW](graphics/PW.png)

### Плюсы:

* прост в реализации
* хорошее качество классификации при правильно подобраном h
* все точки с одинаковым расстоянием будут учитаны
* классификация точки занимает  , так как не требует сортировки

### Минусы:

* необходимо хранить всю выборку целиком
* бедный набор параметров
* в случае одинаковых весов классов алгоритм выбирает любой (однако стоит заметить, что эти случаи будут встречаться редко)
* диапазон параметра h необходимо подбирать самостоятельно, учитывая плотность расположения точек
* если ни одна точка не попала в радиус h, алгоритм не способен ее классифицировать (не актуально для гауссовского ядра)

## STOLP

Отступом (margin) объекта ![](https://latex.codecogs.com/svg.latex?%5Clarge%20xi%20%5Csubseteq%20X_l) относительно алгоритма классифи-
кации, имеющего вид ![](https://latex.codecogs.com/svg.latex?%5Clarge%20a%28u%29%20%3D%20arg%20%5Cmax_%7By%5Csubseteq%20Y%7D%5CGamma_y%28u%29), называется величина
![](https://latex.codecogs.com/svg.latex?%5Clarge%20M%28x_i%29%20%3D%20%5CGamma_%7By_i%7D%28x_i%29%20-%20%5Cmax_%7By%5Csubseteq%20Y%7D%5CGamma_y%28x_i%29).

Отступ показывает степень типичности объекта. Отступ отрицателен тогда
и только тогда, когда алгоритм допускает ошибку на данном объекте.
В зависимости от значений отступа обучающие объекты условно делятся на
пять типов, в порядке убывания отступа: эталонные, неинформативные, погранич-
ные, ошибочные, шумовые.

### STOLP для kwNN
![](graphics/STOLP4kwNN.png)


